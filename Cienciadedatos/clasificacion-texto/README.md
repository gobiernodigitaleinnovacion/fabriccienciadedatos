Ejercicio 5: Clasificaci√≥n de Texto para Determinar el G√©nero de Libros üìö

En este proyecto, desarroll√© un modelo de clasificaci√≥n de texto para determinar el g√©nero de libros (ficci√≥n o no ficci√≥n) basado √∫nicamente en sus t√≠tulos, utilizando datos de la British Library. Implement√© el flujo de trabajo completo en Microsoft Fabric con Apache Spark y MLflow, empleando t√©cnicas de procesamiento de lenguaje natural (NLP) como word2vec para la vectorizaci√≥n de texto y regresi√≥n log√≠stica para la clasificaci√≥n. A continuaci√≥n, detallo el proceso, los resultados y las lecciones aprendidas.
üéØ Objetivo
El objetivo principal fue construir un modelo de aprendizaje autom√°tico capaz de clasificar libros como ficci√≥n o no ficci√≥n bas√°ndose √∫nicamente en sus t√≠tulos. Este proyecto forma parte de una serie de ejercicios de ciencia de datos realizados en Microsoft Fabric, demostrando un flujo de trabajo integral de ciencia de datos: desde la carga y exploraci√≥n de datos hasta el entrenamiento, evaluaci√≥n y predicci√≥n con un modelo.
üìä Dataset
El dataset proviene de la British Library y contiene metadatos de libros digitalizados, incluyendo sus t√≠tulos y etiquetas de g√©nero (Fiction o Non-fiction) asignadas manualmente. Algunas caracter√≠sticas clave del dataset:

Archivo: blbooksgenre.csv
Tama√±o: Miles de registros con t√≠tulos y etiquetas de g√©nero.
Columnas relevantes:
Title: T√≠tulo del libro (texto a clasificar).
annotator_genre: Etiqueta de g√©nero (Fiction o Non-fiction).


Desaf√≠os:
Los t√≠tulos son cortos y a menudo contienen ruido (puntuaci√≥n, palabras irrelevantes).
Desbalance inicial entre las clases, que fue corregido durante el preprocesamiento.



üõ†Ô∏è Proceso T√©cnico
1. Carga y Limpieza de Datos

Carga: El dataset (blbooksgenre.csv) se descarg√≥ desde una URL p√∫blica y se almacen√≥ en el lakehouse de Microsoft Fabric (textclassificationlakehouse) en la ruta Files/title-genre-classification/raw/.
Limpieza:
Se seleccionaron las columnas relevantes (Title y annotator_genre).
Se filtraron las etiquetas v√°lidas (Fiction, Non-fiction) y se eliminaron duplicados basados en los t√≠tulos.
Se balancearon las clases usando ClassBalancer de SynapseML para mitigar el desbalance inicial (peso de Fiction ajustado a 2.956).



2. An√°lisis Exploratorio de Datos (EDA)

Tokenizaci√≥n y eliminaci√≥n de stopwords: Utilic√© Tokenizer y StopWordsRemover de PySpark ML para procesar los t√≠tulos, dividi√©ndolos en tokens y eliminando palabras comunes (stopwords) que no aportan valor sem√°ntico.
Visualizaci√≥n con nubes de palabras:
Gener√© nubes de palabras para cada clase usando la librer√≠a wordcloud:
Fiction: Palabras como "poem", "novel", "tale" predominan, reflejando narrativas y creatividad.
Non-fiction: Palabras como "history", "study", "bibliographical" son frecuentes, indicando temas hist√≥ricos o acad√©micos.


Estas visualizaciones se guardaron como wordcloud_fiction.png y wordcloud_nonfiction.png.



3. Vectorizaci√≥n del Texto

Word2Vec: Utilic√© Word2Vec de PySpark ML para convertir los tokens filtrados en vectores de 128 dimensiones (word2vec_size=128). Se configur√≥ un conteo m√≠nimo de palabras (min_word_count=3) para ignorar t√©rminos poco frecuentes.
Transformaci√≥n de etiquetas: Las etiquetas de g√©nero se convirtieron a √≠ndices num√©ricos usando StringIndexer (Fiction ‚Üí 1.0, Non-fiction ‚Üí 0.0).
Resultado: Un DataFrame con columnas Title, annotator_genre, features (vectores), labelIdx (etiquetas num√©ricas) y weight (peso de balanceo).

4. Entrenamiento del Modelo

Divisi√≥n del dataset: Se dividi√≥ en 80% para entrenamiento y 20% para prueba (randomSplit([0.8, 0.2], seed=42)).
Modelo: Entren√© un modelo de regresi√≥n log√≠stica (LogisticRegression) con las siguientes configuraciones:
maxIter=10: M√°ximo de 10 iteraciones.
Caracter√≠sticas: Vectores de word2vec (features).
Etiquetas: labelIdx.
Peso: weight (para balanceo de clases).


Optimizaci√≥n de hiperpar√°metros:
Utilic√© validaci√≥n cruzada (CrossValidator) con 3 pliegues (numFolds=3).
Cuadr√≠cula de hiperpar√°metros: regParam=[0.03, 0.1], elasticNetParam=[0.0, 0.1].


M√©tricas de evaluaci√≥n: areaUnderROC y areaUnderPR (evaluador binario, ya que hay dos clases).

5. Evaluaci√≥n del Modelo

Resultados de los submodelos:
Submodelo 1 (regParam=0.03, elasticNetParam=0.0): areaUnderROC=0.7698, areaUnderPR=0.7115.
Submodelo 2 (regParam=0.03, elasticNetParam=0.1): areaUnderROC=0.7435.
Submodelo 3 (regParam=0.1, elasticNetParam=0.0): areaUnderROC=0.7565, areaUnderPR=0.6888.
Submodelo 4 (regParam=0.1, elasticNetParam=0.1): areaUnderROC=0.7337, areaUnderPR=0.6631.


Mejor modelo: El submodelo 1 (regParam=0.03, elasticNetParam=0.0) fue seleccionado por tener el mayor areaUnderROC (0.7698).

6. Predicciones y Almacenamiento

Predicciones por lotes: Us√© el mejor modelo (versi√≥n 5 en MLflow) para generar predicciones en el conjunto de prueba.
Ejemplo de predicciones:
T√≠tulo: 'Harpstrings.' A poem ‚Üí Predicci√≥n: Fiction (correcto).
T√≠tulo: A Balk√°n f√©lsziget... ‚Üí Predicci√≥n: Fiction (incorrecto, deber√≠a ser Non-fiction).


Almacenamiento: Las predicciones se guardaron en el lakehouse en Files/title-genre-classification/predictions/batch_predictions como archivos Delta.

7. Tiempo de ejecuci√≥n

El notebook completo tom√≥ 2085 segundos (~34 minutos), lo que incluye la instalaci√≥n de librer√≠as, preprocesamiento, entrenamiento, evaluaci√≥n y predicci√≥n.

üìà Resultados y Conclusiones

Patrones identificados:
Las nubes de palabras revelaron diferencias claras entre g√©neros: los t√≠tulos de ficci√≥n suelen incluir t√©rminos narrativos, mientras que los de no ficci√≥n tienen un enfoque m√°s acad√©mico o hist√≥rico.


Desempe√±o del modelo:
El mejor modelo logr√≥ un areaUnderROC de 0.7698 y un areaUnderPR de 0.7115, indicando un desempe√±o aceptable para un problema de clasificaci√≥n binaria.
Sin embargo, el modelo comete errores (por ejemplo, clasificando algunos t√≠tulos de no ficci√≥n como ficci√≥n), lo que sugiere que los t√≠tulos cortos pueden no ser suficientes para una clasificaci√≥n precisa.


Lecciones aprendidas:
La vectorizaci√≥n con word2vec captura bien el contexto sem√°ntico, pero el tama√±o del vector (word2vec_size=128) y el conteo m√≠nimo de palabras (min_word_count=3) podr√≠an ajustarse para mejorar el modelo.
Los t√≠tulos cortos son un desaf√≠o para la clasificaci√≥n; incorporar m√°s contexto (como res√∫menes o metadatos adicionales) podr√≠a mejorar los resultados.
El balanceo de clases fue crucial para mitigar el desbalance inicial, pero podr√≠a explorarse otras t√©cnicas como SMOTE.



üõ†Ô∏è Tecnolog√≠as Utilizadas

Entorno: Microsoft Fabric (Workspace: clasificacion-texto, Lakehouse: textclassificationlakehouse).
Librer√≠as:
PySpark: Para procesamiento distribuido de datos.
SynapseML: Para balanceo de clases (ClassBalancer).
MLflow: Para rastreo de experimentos y registro de modelos.
Word2Vec y LogisticRegression: Para vectorizaci√≥n y clasificaci√≥n (PySpark ML).
WordCloud, Seaborn, Matplotlib: Para visualizaci√≥n.
Python: Lenguaje principal.



üìÇ Estructura del Repositorio
Ejercicio-5-Clasificacion-Texto/
‚îú‚îÄ‚îÄ text_classification.ipynb    # Notebook con el c√≥digo completo
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ blbooksgenre.csv         # Dataset original
‚îÇ   ‚îú‚îÄ‚îÄ batch_predictions.csv    # Predicciones generadas
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îú‚îÄ‚îÄ wordcloud_fiction.png    # Nube de palabras para Fiction
‚îÇ   ‚îú‚îÄ‚îÄ wordcloud_nonfiction.png # Nube de palabras para Non-fiction
‚îú‚îÄ‚îÄ README.md                    # Este archivo

üöÄ ¬øC√≥mo Reproducir Este Proyecto?

Configura el entorno:
Crea una carpeta clasificacion-texto en Microsoft Fabric.
A√±ade un lakehouse (textclassificationlakehouse).
Crea un notebook (text_classification.ipynb) y vinc√∫lalo al lakehouse.


Ejecuta el notebook:
Sigue los bloques de c√≥digo en orden (instalaci√≥n, carga de datos, EDA, entrenamiento, evaluaci√≥n, predicciones).
Aseg√∫rate de guardar las visualizaciones generadas (wordcloud_fiction.png, wordcloud_nonfiction.png).


Descarga los archivos:
Descarga el notebook, el dataset, las predicciones y las gr√°ficas siguiendo las instrucciones del bloque 6.


Explora los resultados:
Revisa las m√©tricas (areaUnderROC, areaUnderPR) y las nubes de palabras para entender los patrones en los datos.



üåü Reflexi√≥n
Este proyecto fue una excelente oportunidad para explorar t√©cnicas de NLP y clasificaci√≥n en un entorno distribuido como Microsoft Fabric. Aunque el modelo mostr√≥ un desempe√±o prometedor, trabajar con t√≠tulos cortos destac√≥ las limitaciones de este enfoque y la importancia de un buen preprocesamiento y selecci√≥n de caracter√≠sticas. En el futuro, me gustar√≠a experimentar con modelos m√°s avanzados (como transformers) y datos adicionales para mejorar la precisi√≥n.
üë§ Autor: Juan Heriberto Rosas Ju√°rezüìß Correo: juanheriberto.rosas@jhrjdata.comüåê LinkedIn: Juan Heriberto Rosas Ju√°rezüè¢ Organizaci√≥n: Gobierno Digital e Innovaci√≥n
